import pandas as pd
from io import BytesIO
from sqlalchemy.orm import Session
from app import models
from sqlalchemy import func
from collections import Counter
from datetime import datetime
from datetime import timedelta

# --- 1. T·ª™ ƒêI·ªÇN MAPPING (C·∫¨P NH·∫¨T THEO FILE TH·ª∞C T·∫æ) ---
PLATFORM_MAPPING = {
    'SHOPEE': {
        # f35Wh2: M√£ h√≥a c·ªßa n·ªôi dung comment Shopee
        'content_cols': ['YNedDV', 'content', 'comment', 'shopee-product-rating__content'],
        'author_cols': ['InK5kS', 'author', 'name', 'shopee-product-rating__author-name'],
        'time_cols': ['XYk98l', 'time', 'date', 'shopee-product-rating__time'],
        'likes_cols': ['shopee-product-rating__like-count', 'like']
    },
    'FACEBOOK': {
        # xdj266r: M√£ h√≥a c·ªßa n·ªôi dung comment Facebook
        'content_cols': ['xdj266r', 'content', 'message', 'text'],
        'author_cols': ['x193iq5w', 'author', 'name', 'user'],
        'time_cols': ['x1i10hfl', 'time', 'date'],
        'likes_cols': ['html-span', 'likes', 'reaction']
    },
    'OTHER': {
        'content_cols': ['content', 'comment', 'review', 'text', 'noidung', 'feedback'],
        'author_cols': ['user', 'name', 'author', 'khachhang'],
        'time_cols': ['time', 'date', 'ngay'],
        'likes_cols': ['like', 'thich']
    }
}

# --- 2. H√ÄM TR·ª¢ GI√öP T√åM C·ªòT ---
def find_column(df_columns, possible_names):
    """T√¨m t√™n c·ªôt trong CSV kh·ªõp v·ªõi c·∫•u h√¨nh"""
    df_cols_lower = {col.lower().strip(): col for col in df_columns}
    
    for name in possible_names:
        name_lower = name.lower()
        if name_lower in df_cols_lower:
            return df_cols_lower[name_lower]
            
    return None

# --- 3. H√ÄM T·∫†O FEEDBACK (T√°ch ra ƒë·ªÉ t√°i s·ª≠ d·ª•ng) ---
def create_feedback_with_analysis(db: Session, content: str, source_id: int = 3, custom_time: datetime = None):
    from app import services # Import ·ªü ƒë√¢y ƒë·ªÉ tr√°nh circular import
    """
    T·∫°o Feedback. N·∫øu c√≥ custom_time (t·ª´ Extension) th√¨ d√πng, 
    n·∫øu kh√¥ng th√¨ ƒë·ªÉ Database t·ª± l·∫•y gi·ªù hi·ªán t·∫°i.
    """
    db_feedback = models.Feedback(
        raw_content=content,
        source_id=source_id,
        status="PROCESSED"
    )

    # üëá LOGIC QUAN TR·ªåNG: Ghi ƒë√® th·ªùi gian
    if custom_time:
        db_feedback.received_at = custom_time
        
    db.add(db_feedback)
    db.commit()
    db.refresh(db_feedback)

    # ... (Ph·∫ßn g·ªçi AI gi·ªØ nguy√™n kh√¥ng ƒë·ªïi) ...
    source_name = "OTHER"
    if source_id == 1: source_name = "FACEBOOK"
    elif source_id == 2: source_name = "SHOPEE"
    
    ai_result = services.analyze_text(content, source=source_name)
    
    db_analysis = models.AnalysisResult(
        feedback_id=db_feedback.id,
        sentiment_score=ai_result['score'],
        sentiment_label=ai_result['label'],
        keywords=ai_result['keywords']
    )
    db.add(db_analysis)
    db.commit()
    
    return db_feedback

# --- 4. H√ÄM X·ª¨ L√ù CSV ---
def process_csv_upload(db: Session, file_contents: bytes, platform: str = 'OTHER'):
    try:
        # ƒê·ªçc file CSV
        df = pd.read_csv(BytesIO(file_contents))
        
        # L·∫•y c·∫•u h√¨nh mapping
        config = PLATFORM_MAPPING.get(platform.upper(), PLATFORM_MAPPING['OTHER'])
        
        # T√¨m c·ªôt n·ªôi dung (B·∫Øt bu·ªôc ph·∫£i c√≥)
        content_col = find_column(df.columns, config['content_cols'])
        
        if not content_col:
            print(f"‚ùå [Platform: {platform}] Kh√¥ng t√¨m th·∫•y c·ªôt n·ªôi dung. C·ªôt hi·ªán c√≥: {list(df.columns)}")
            return

        print(f"üöÄ [{platform}] ƒê√£ map c·ªôt '{content_col}' l√† n·ªôi dung. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
        
        # T√¨m c√°c c·ªôt ph·ª• (Metadata)
        author_col = find_column(df.columns, config['author_cols'])
        time_col = find_column(df.columns, config['time_cols'])
        likes_col = find_column(df.columns, config['likes_cols'])

        count = 0
        for _, row in df.iterrows():
            raw_text = row[content_col]
            
            # B·ªè qua d√≤ng tr·ªëng
            if pd.isna(raw_text) or str(raw_text).strip() == "":
                continue
                
            text = str(raw_text)
            
            # Gom th√¥ng tin ph·ª• v√†o JSON
            customer_meta = {"imported_from": platform}
            
            if author_col and pd.notna(row[author_col]): 
                customer_meta['name'] = str(row[author_col])
                
            if time_col and pd.notna(row[time_col]):
                # Shopee hay c√≥ ki·ªÉu "2023-08-20 | Ph√¢n lo·∫°i...", ta ch·ªâ l·∫•y ng√†y gi·ªù ƒë·∫ßu
                time_val = str(row[time_col])
                if platform == 'SHOPEE' and '|' in time_val:
                    time_val = time_val.split('|')[0].strip()
                customer_meta['time_posted'] = time_val
                
            if likes_col and pd.notna(row[likes_col]):
                customer_meta['likes'] = str(row[likes_col])

            # L∆∞u v√†o DB
            try:
                src_id = 3 # M·∫∑c ƒë·ªãnh l√† Other
                
                if platform == 'FACEBOOK':
                    src_id = 1
                elif platform == 'SHOPEE':
                    src_id = 2
                
                # G·ªçi h√†m t·∫°o
                db_feedback = create_feedback_with_analysis(db, text, source_id=src_id)
                
                # Update metadata
                db_feedback.customer_info = customer_meta
                db.commit()
                
                count += 1
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói d√≤ng: {e}")
                continue

        print(f"‚úÖ ƒê√£ x·ª≠ l√Ω xong {count} d√≤ng d·ªØ li·ªáu t·ª´ {platform}.")

    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc file CSV: {e}")

# --- C√°c h√†m CRUD kh√°c gi·ªØ nguy√™n ---
def get_feedbacks(db: Session, skip: int = 0, limit: int = 100):
    return db.query(models.Feedback).order_by(models.Feedback.received_at.desc()).offset(skip).limit(limit).all()

def get_stats(db: Session):
    total = db.query(models.Feedback).count()
    rows = db.query(
        models.AnalysisResult.sentiment_label, 
        func.count(models.AnalysisResult.sentiment_label)
    ).group_by(models.AnalysisResult.sentiment_label).all()
    
    return {
        "total": total,
        "sentiment_counts": {r[0]: r[1] for r in rows}
    }

def get_keyword_stats(db: Session, limit: int = 50):
    """
    L·∫•y danh s√°ch t·ª´ kh√≥a xu·∫•t hi·ªán nhi·ªÅu nh·∫•t ƒë·ªÉ v·∫Ω Word Cloud
    """
    # 1. L·∫•y to√†n b·ªô c·ªôt keywords t·ª´ b·∫£ng AnalysisResult
    # Ch·ªâ l·∫•y c√°c d√≤ng c√≥ keywords (kh√¥ng null)
    results = db.query(models.AnalysisResult.keywords)\
                .filter(models.AnalysisResult.keywords != None).all()
    
    # 2. L√†m ph·∫≥ng list (Flatten): [[a, b], [b, c]] -> [a, b, b, c]
    all_keywords = []
    for row in results:
        # row l√† tuple, row[0] l√† list keywords
        if row[0]: 
            all_keywords.extend(row[0])
            
    # 3. ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán
    counter = Counter(all_keywords)
    
    # 4. L·∫•y top keywords ph·ªï bi·∫øn nh·∫•t
    most_common = counter.most_common(limit)
    
    # 5. Format l·∫°i theo chu·∫©n Frontend c·∫ßn: { value: 't·ª´ kh√≥a', count: 10 }
    return [{"value": word, "count": count} for word, count in most_common]

def update_analysis_result(db: Session, feedback_id: str, new_label: str):
    # 1. T√¨m b·∫£n ghi AnalysisResult d·ª±a tr√™n feedback_id
    analysis = db.query(models.AnalysisResult).filter(
        models.AnalysisResult.feedback_id == feedback_id
    ).first()
    
    if not analysis:
        return None
        
    # 2. C·∫≠p nh·∫≠t nh√£n m·ªõi
    analysis.sentiment_label = new_label
    
    # 3. C·∫≠p nh·∫≠t l·∫°i ƒëi·ªÉm s·ªë (Score) cho kh·ªõp logic (Optional)
    # N·∫øu s·ª≠a th√†nh POSITIVE th√¨ set ƒëi·ªÉm l√† 0.9, NEGATIVE l√† -0.9
    if new_label == "POSITIVE":
        analysis.sentiment_score = 0.9
    elif new_label == "NEGATIVE":
        analysis.sentiment_score = -0.9
    else:
        analysis.sentiment_score = 0.0
        
    db.commit()
    db.refresh(analysis)
    return analysis

def get_customer_profiles(db: Session, skip: int = 0, limit: int = 10):
    # 1. L·∫•y to√†n b·ªô d·ªØ li·ªáu (ch·ªâ l·∫•y c√°c c·ªôt c·∫ßn thi·∫øt cho nh·∫π)
    feedbacks = db.query(models.Feedback).all()

    if not feedbacks:
        return [], 0

    # 2. Chuy·ªÉn sang Pandas DataFrame
    data = []
    for f in feedbacks:
        # L·∫•y t√™n kh√°ch h√†ng an to√†n
        name = "Anonymous"
        if f.customer_info and "name" in f.customer_info:
            name = f.customer_info["name"]

        data.append({
            "name": name,
            "source_id": f.source_id,
            "score": f.analysis.sentiment_score if f.analysis else 0,
            "label": f.analysis.sentiment_label if f.analysis else "NEUTRAL",
            "date": f.received_at
        })

    df = pd.DataFrame(data)

    # 3. Gom nh√≥m theo T√™n (Group By)
    profiles = []
    # Group by Name v√† Source (ƒë·ªÉ tr√°nh tr√πng t√™n nh∆∞ng kh√°c ngu·ªìn)
    grouped = df.groupby(['name', 'source_id'])

    for (name, source_id), group in grouped:
        total = len(group)
        # T√≠nh t·ª∑ l·ªá t√≠ch c·ª±c
        pos_count = len(group[group['label'] == 'POSITIVE'])
        pos_ratio = round(pos_count / total, 2)

        # T√≠nh ƒëi·ªÉm trung b√¨nh
        avg_score = round(group['score'].mean(), 2)

        # ƒê√°nh gi√° xu h∆∞·ªõng
        if avg_score > 0.5: trend = "Fan c·ª©ng"
        elif avg_score < -0.3: trend = "Kh√≥ t√≠nh"
        else: trend = "Trung l·∫≠p"

        profiles.append({
            "name": name,
            "source_id": source_id,
            "total_comments": total,
            "positive_ratio": pos_ratio,
            "avg_score": avg_score,
            "last_interaction": str(group['date'].max()), # L·∫ßn cu·ªëi xu·∫•t hi·ªán
            "sentiment_trend": trend
        })

    # S·∫Øp x·∫øp: Ai comment nhi·ªÅu nh·∫•t l√™n ƒë·∫ßu
    profiles.sort(key=lambda x: x['total_comments'], reverse=True)

    # 4. √Åp d·ª•ng pagination
    total_count = len(profiles)
    paginated_profiles = profiles[skip:skip + limit]

    return paginated_profiles, total_count

def get_customer_history(db: Session, customer_name: str, limit: int = 20):
    """
    L·∫•y l·ªãch s·ª≠ comment c·ªßa m·ªôt kh√°ch h√†ng c·ª• th·ªÉ (d·ª±a tr√™n t√™n).
    L·∫•y t·ªëi ƒëa 50 comment g·∫ßn nh·∫•t ƒë·ªÉ ti·∫øt ki·ªám token cho AI.
    """
    # V√¨ JSONB l∆∞u tr·ªØ linh ƒë·ªông, ta d√πng filter tr√™n field customer_info
    # L∆∞u √Ω: C√°ch query JSONB c√≥ th·ªÉ kh√°c nhau t√πy version Postgres, 
    # c√°ch ƒë∆°n gi·∫£n nh·∫•t l√† l·∫•y h·∫øt r·ªìi filter Python (v·ªõi d·ªØ li·ªáu nh·ªè <100k d√≤ng)
    # C√°ch t·ªëi ∆∞u h∆°n l√† d√πng SQL operator ->>
    
    # C√°ch an to√†n cho ƒë·ªì √°n (Filter Python):
    all_feedbacks = db.query(models.Feedback).order_by(models.Feedback.received_at.desc()).all()
    
    history = []
    for f in all_feedbacks:
        if f.customer_info and f.customer_info.get("name") == customer_name:
            history.append({
                "content": f.raw_content,
                "date": str(f.received_at),
                "source": f.customer_info.get("imported_from"),
                "label": f.analysis.sentiment_label if f.analysis else "Unknown"
            })
            if len(history) >= limit:
                break
    return history

def get_sentiment_trend(db: Session, days: int = 7):
    """
    Th·ªëng k√™ xu h∆∞·ªõng (T·ªëi ∆∞u h√≥a + X·ª≠ l√Ω ng√†y tr·ªëng)
    """
    # 1. X√°c ƒë·ªãnh khung th·ªùi gian
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days - 1) # L·∫•y ƒë·ªß range
    
    # 2. Query d·ªØ li·ªáu th√¥ (Ch·ªâ l·∫•y c·ªôt c·∫ßn thi·∫øt cho nh·∫π)
    # L·ªçc c√°c b·∫£n ghi trong kho·∫£ng th·ªùi gian
    feedbacks = db.query(models.Feedback.received_at, models.AnalysisResult.sentiment_label)\
        .join(models.AnalysisResult)\
        .filter(models.Feedback.received_at >= start_date)\
        .all()
    
    # 3. T·∫°o khung x∆∞∆°ng ng√†y th√°ng ƒë·∫ßy ƒë·ªß (Full Date Range)
    # ƒê·ªÉ ƒë·∫£m b·∫£o ng√†y n√†o c≈©ng hi·ªÉn th·ªã, k·ªÉ c·∫£ ng√†y kh√¥ng c√≥ comment
    idx = pd.date_range(start=start_date, end=end_date, freq='D').normalize()
    
    # Chu·∫©n b·ªã c·∫•u tr√∫c d·ªØ li·ªáu m·∫∑c ƒë·ªãnh (to√†n s·ªë 0)
    final_data = {
        "dates": idx.strftime('%d/%m').tolist(),
        "positive": [0] * len(idx),
        "negative": [0] * len(idx),
        "neutral": [0] * len(idx)
    }

    if not feedbacks:
        return final_data

    # 4. X·ª≠ l√Ω d·ªØ li·ªáu b·∫±ng Pandas
    try:
        data = [{"date": f.received_at, "label": f.sentiment_label} for f in feedbacks]
        df = pd.DataFrame(data)
        
        # Convert sang datetime v√† b·ªè ph·∫ßn gi·ªù ph√∫t (normalize) ƒë·ªÉ group theo ng√†y
        df['date'] = pd.to_datetime(df['date']).dt.normalize()
        
        # Gom nh√≥m: ƒê·∫øm s·ªë l∆∞·ª£ng theo Ng√†y + Nh√£n
        # size() ƒë·∫øm s·ªë d√≤ng, unstack(fill_value=0) ƒë·ªÉ xoay b·∫£ng v√† ƒëi·ªÅn 0 v√†o √¥ tr·ªëng
        grouped = df.groupby(['date', 'label']).size().unstack(fill_value=0)
        
        # Reindex: √âp b·∫£ng d·ªØ li·ªáu ph·∫£i kh·ªõp v·ªõi khung x∆∞∆°ng idx ƒë√£ t·∫°o ·ªü b∆∞·ªõc 3
        # fill_value=0: N·∫øu ng√†y ƒë√≥ trong DB kh√¥ng c√≥, ƒëi·ªÅn s·ªë 0
        grouped = grouped.reindex(idx, fill_value=0)
        
        # 5. Tr√≠ch xu·∫•t d·ªØ li·ªáu an to√†n
        # Ki·ªÉm tra xem c·ªôt c√≥ t·ªìn t·∫°i kh√¥ng, n·∫øu kh√¥ng th√¨ l·∫•y m·∫£ng 0
        if 'POSITIVE' in grouped.columns:
            final_data['positive'] = grouped['POSITIVE'].tolist()
            
        if 'NEGATIVE' in grouped.columns:
            final_data['negative'] = grouped['NEGATIVE'].tolist()
            
        if 'NEUTRAL' in grouped.columns:
            final_data['neutral'] = grouped['NEUTRAL'].tolist()
            
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω bi·ªÉu ƒë·ªì: {e}")
        # N·∫øu l·ªói v·∫´n tr·∫£ v·ªÅ data r·ªóng ƒë·ªÉ frontend kh√¥ng ch·∫øt
        return final_data

    return final_data